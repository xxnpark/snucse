\documentclass[10pt]{article}
%\usepackage[left=2.3cm,right=2.3cm,top=2.5cm,bottom=3cm,a4paper]{geometry}
\usepackage{fullpage}
\usepackage{setspace}
\setstretch{1.3}
\usepackage{amsmath,amssymb,amsthm,physics,units,mathtools}
\usepackage{mdwlist}
\usepackage{paralist}
\setlength\parindent{0pt}
\usepackage{float}
\usepackage{xcolor}
\usepackage{algorithm,algpseudocode}
\usepackage{listings}
\usepackage[colorlinks=true]{hyperref}
\NewDocumentCommand{\code}{v}{%
\texttt{#1}%
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{lgray}{RGB}{240,239,239}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
    % frame=tb,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle=\ttfamily,
    numbers=none,
    backgroundcolor=\color{lgray},
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},                
    keepspaces=true,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\begin{document}
\begin{center}
    {\LARGE MathDNN Homework 7} \\
\end{center}
\begin{flushright}
    Department of Computer Science and Engineering \\
    2021-16988 Jaewan Park
\end{flushright}

\section*{Problem 1}
\begin{enumerate}[(a)]
    \item WLOG, assume $x_1 \geq x_2 \geq \cdots \geq x_n$. 
    Then 
    \begin{align*}
        \nu_\beta(x) &= \frac{1}{\beta}\log\sum_{i=1}^{n}\exp\qty(\beta x_i) = \frac{1}{\beta}\log\qty(\exp\qty(\beta x_1) + \exp\qty(\beta x_2) + \cdots + \exp\qty(\beta x_n)) \\
        &\leq \frac{1}{\beta}\log \qty(n\exp\qty(\beta x_1)) = x_1 + \frac{\log{n}}{\beta} \\
        \nu_\beta(x) &= \frac{1}{\beta}\log\sum_{i=1}^{n}\exp\qty(\beta x_i) = \frac{1}{\beta}\log\qty(\exp\qty(\beta x_1) + \exp\qty(\beta x_2) + \cdots + \exp\qty(\beta x_n)) \\
        &\geq \frac{1}{\beta}\log\qty(\exp\qty(\beta x_1)) = x_1 \\
    \end{align*}
    so $\displaystyle \lim_{\beta \to \infty}x_1 \leq \lim_{\beta \to \infty}\nu_\beta(x) \leq \lim_{\beta \to \infty}\qty(x_1 + \frac{\log{n}}{\beta})$, therefore
    $$\displaystyle \lim_{\beta \to \infty}\nu_\beta(x) = x_1 = \max\qty{x_1, \cdots, x_n}.$$
    \item The partial derivatives of $\nu_1(x)$ are
    \begin{align*}
        \frac{\partial\nu_1}{\partial x_j} &= \frac{\partial}{\partial x_j}\log\sum_{i=1}^{n}\exp\qty(x_i) = \frac{\exp\qty(x_j)}{\sum_{i=1}^{n}\exp\qty(x_i)} = \mu_j.
    \end{align*}
    Therefore $\nabla\nu_1 = \mu$.
    \item The partial derivatives of $\nu_\beta(x)$ are
    \begin{align*}
        \frac{\partial\nu_\beta}{\partial x_j} &= \frac{\partial}{\partial x_j}\qty(\frac{1}{\beta}\log\sum_{i=1}^{n}\exp\qty(\beta x_i)) = \frac{\exp\qty(\beta x_j)}{\sum_{i=1}^{n}\exp\qty(\beta x_i)}.
    \end{align*}
    When $\beta \to \infty$, we obtain 
    \begin{align*}
        \lim_{\beta \to \infty}\frac{\partial\nu_\beta}{\partial x_j} &= \lim_{\beta \to \infty}\frac{\exp\qty(\beta x_j)}{\sum_{i=1}^{n}\exp\qty(\beta x_i)} = \lim_{\beta \to \infty}\frac{1}{\sum_{i=1}^{n}\exp\qty(\beta x_i - \beta x_j)} \\
        &= \begin{cases}
            1 & \qty(j = i_{\max}) \\
            0 & \qty(j \neq i_{\max})
        \end{cases}.
    \end{align*}
    Therefore $\displaystyle \lim_{\beta \to \infty}\nabla\nu_\beta(x) = \mathbf{e}_{i_{\max}}$.
\end{enumerate}

\section*{Problem 2}
The number of additions and multiplications in convolutional layers are both $C_{\mathrm{in}} \times F^2 \times C_{\mathrm{out}} \times h_{\mathrm{out}}^2$, where $F \times F$ is the kernel size and $h_{\mathrm{out}} \times h_{\mathrm{out}}$ is the output dimension.
The number of additions and multiplications in linear layers are both $n_{\mathrm{in}} \times n_{\mathrm{out}}$, where $n_{\mathrm{in}}$ and $n_{\mathrm{out}}$ are each input and output vector sizes.
Additions are made between multiplied values, so the number of it should be one less than that of multiplications, but the process of adding the bias makes them equal.

\vspace{3mm}
Therefore the number of operations needed in the convolutional layers is
\begin{align*}
    &2 \times (3 \times 11^2 \times 64 \times 55^2 + 64 \times 5^2 \times 192 \times 27^2 + 192 \times 3^2 \times 384 \times 13^2 \\ 
    &\hspace*{4cm} + 384 \times 3^2 \times 256 \times 13^2 + 256 \times 3^2 \times 256 \times 13^2) = 1311133056
\end{align*}
and the number of operations needed in the linear layers is
\begin{align*}
    2 \times (256 \times 6^2 \times 4096 + 4096 \times 4096 + 4096 \times 1000) = 117243904.
\end{align*}

\section*{Problem 4}
\noindent\fbox{%
\parbox{0.98\textwidth}{%
    \textit{Notation} For a matrix or vector $X$, the notation $[X]_{i, j}$ refers to the element of $X$ at that index, and $\qty{f(i, j)}_{i, j}$ refers to the matrix of which element at index $(i, j)$ is $f(i, j)$.
}%
}
\begin{enumerate}[(a)]
    \item It is obvious that
    \begin{align*}
        \frac{\partial y_L}{\partial y_{L-1}} = \frac{\partial}{\partial y_{L-1}}\qty(A_{w_L}y_{L-1} + b_L\mathbf{1}_{n_L}) = A_{w_L}.
    \end{align*}
    For $\ell = 2, \cdots, L-1$, we obtain
    \begin{align*}
        \frac{\partial y_\ell}{\partial y_{\ell-1}} &= \frac{\partial}{\partial y_{\ell-1}}\sigma\qty(A_{w_\ell}y_{\ell-1} + b_\ell\mathbf{1}_{n_\ell}) \\
        &= \qty{\frac{\partial}{\partial\qty[y_{\ell-1}]_j}\qty[\sigma\qty(A_{w_\ell}y_{\ell-1} + b_\ell\mathbf{1}_{n_\ell})]_i}_{i, j} = \qty{\frac{\partial}{\partial\qty[y_{\ell-1}]_j}\sigma\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)}_{i, j} \\
        &= \qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\frac{\partial}{\partial\qty[y_{\ell-1}]_j}\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)}_{i, j} \\
        &= \qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\frac{\partial}{\partial\qty[y_{\ell-1}]_j}\qty(\sum_{k=1}^{n_{\ell-1}}\qty[A_{w_\ell}]_{i, k}\qty[y_{\ell-1}]_k + b_\ell)}_{i, j} \\
        &= \qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\qty[A_{w_\ell}]_{i, j}}_{i, j} \\
        &= \mathrm{diag}\qty(\sigma'\qty(A_{w_\ell}y_{\ell-1} + b_\ell\mathbf{1}_{n_\ell}))A_{w_\ell}.
    \end{align*}
    $A_{w_\ell}$ will be in the form of the following.
    $$A_{w_\ell} = \begin{bmatrix}
        \qty[w_\ell]_1 & \cdots         & \qty[w_\ell]_{f_\ell} & 0                     & 0                     & \cdots                & 0      \\
        0              & \qty[w_\ell]_1 & \cdots                & \qty[w_\ell]_{f_\ell} & 0                     & \cdots                & 0      \\
        \vdots         &                & \ddots                &                       & \ddots                &                       & \vdots \\
        0              & \cdots         & 0                     & \qty[w_\ell]_1        & \cdots                & \qty[w_\ell]_{f_\ell} & 0      \\
        0              & \cdots         & 0                     & 0                     & \qty[w_\ell]_1        & \cdots                & \qty[w_\ell]_{f_\ell}
    \end{bmatrix}$$
    Then using the chain rule, for $\ell = 1, \cdots, L$, we obtain the following.
    \begin{align*}
        \frac{\partial y_L}{\partial w_\ell} &= \frac{\partial y_L}{\partial y_\ell}\frac{\partial y_\ell}{\partial w_\ell} = \frac{\partial y_L}{\partial y_\ell}\qty{\frac{\partial\qty[y_\ell]_i}{\partial\qty[w_\ell]_j}}_{i, j} \\
        &= \frac{\partial y_L}{\partial y_\ell}\qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\frac{\partial}{\partial\qty[w_\ell]_j}\qty(\sum_{k=1}^{n_{\ell-1}}\qty[A_{w_\ell}]_{i, k}\qty[y_{\ell-1}]_k + b_\ell)}_{i, j} \\
        &= \frac{\partial y_L}{\partial y_\ell}\qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\sum_{k=1}^{n_{\ell-1}}\frac{\partial\qty[A_{w_\ell}]_{i, k}}{\partial\qty[w_\ell]_j}\qty[y_{\ell-1}]_k}_{i, j} \\
        &= \frac{\partial y_L}{\partial y_\ell}\qty{\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_i + \qty[b_\ell\mathbf{1}_{n_\ell}]_i)\qty[y_{\ell-1}]_{i+j-1}}_{i, j} \\
        &= \qty{\sum_{k=1}^{n_\ell}\qty[\frac{\partial y_L}{\partial y_\ell}]_{i, k}\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_k + \qty[b_\ell\mathbf{1}_{n_\ell}]_k)\qty[y_{\ell-1}]_{k+j-1}}_{i, j} \\
        &= \begin{bmatrix}
            \sum_{k=1}^{n_\ell}\qty[\frac{\partial y_L}{\partial y_\ell}]_{1, k}\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_k + \qty[b_\ell\mathbf{1}_{n_\ell}]_k)\qty[y_{\ell-1}]_{k} \\
            \sum_{k=1}^{n_\ell}\qty[\frac{\partial y_L}{\partial y_\ell}]_{1, k}\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_k + \qty[b_\ell\mathbf{1}_{n_\ell}]_k)\qty[y_{\ell-1}]_{k+1} \\
            \vdots \\
            \sum_{k=1}^{n_\ell}\qty[\frac{\partial y_L}{\partial y_\ell}]_{1, k}\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_k + \qty[b_\ell\mathbf{1}_{n_\ell}]_k)\qty[y_{\ell-1}]_{k+f_\ell-1}
        \end{bmatrix}^\intercal \\
        &= \qty(\begin{bmatrix}
            \qty[v_\ell^\intercal]_1 & \cdots                   & \qty[v_\ell^\intercal]_{n_\ell} & 0                               & 0                        & \cdots                          & 0      \\
            0                        & \qty[v_\ell^\intercal]_1 & \cdots                          & \qty[v_\ell^\intercal]_{n_\ell} & 0                        & \cdots                          & 0      \\
            \vdots                   &                          & \ddots                          &                                 & \ddots                   &                                 & \vdots \\
            0                        & \cdots                   & 0                               & \qty[v_\ell^\intercal]_1        & \cdots                   & \qty[v_\ell^\intercal]_{n_\ell} & 0      \\
            0                        & \cdots                   & 0                               & 0                               & \qty[v_\ell^\intercal]_1 & \cdots                          & \qty[v_\ell^\intercal]_{n_\ell}
        \end{bmatrix}\begin{bmatrix}
            \qty[y_{\ell-1}]_{1} \\ 
            \qty[y_{\ell-1}]_{2} \\ 
            \vdots \\
            \qty[y_{\ell-1}]_{n_{\ell-1}-1} \\ 
            \qty[y_{\ell-1}]_{n_{\ell-1}}
        \end{bmatrix})^\intercal \\ 
        &= \qty(\mathcal{C}_{v_\ell^\intercal}y_{\ell-1})^\intercal \;\; \qty(\because \; \qty[v_\ell^\intercal]_k = \qty[\frac{\partial y_L}{\partial y_\ell}]_{1, k}\sigma'\qty(\qty[A_{w_\ell}y_{\ell-1}]_k + \qty[b_\ell\mathbf{1}_{n_\ell}]_k)) \\ \\
        \frac{\partial y_L}{\partial b_\ell} &= \frac{\partial y_L}{\partial y_\ell}\frac{\partial y_\ell}{\partial b_\ell} \\
        &= \frac{\partial y_L}{\partial y_\ell}\mathrm{diag}\qty(\sigma'\qty(A_{w_\ell}y_{\ell-1} + b_\ell\mathbf{1}_{n_\ell}))\qty(\frac{\partial}{\partial b_\ell}\qty(A_{w_\ell}y_{\ell-1} + b_\ell\mathbf{1}_{n_\ell})) \\
        &= v_\ell\mathbf{1}_{n_\ell}
    \end{align*}
    \item Performing multiplications $A_{w_i}v$ or $vA_{w_i}$, where $v$ is a vector of appropriate size, should be computed by performing convolution each.
    $vA_{w_i}$ can be computed according to the equation $vA_{w_i} = \qty(A_{w_i}^\intercal v^\intercal)^\intercal$, so it should be computed by transpose-convolution.
    Instead of considering $A_{w_i}$ or $A_{w_i}^\intercal$ as matrices each, we can perform the process by applying convolution to the target vector with respect to the convolutional filter $w_i$.
    During backprop, applying convolutions are performed sequentially regarding the chain rule.
\end{enumerate}

\end{document}