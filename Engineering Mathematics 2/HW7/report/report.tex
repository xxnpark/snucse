\documentclass{article}
\usepackage{setspace}
\usepackage{bm}
\usepackage{amsmath, amsfonts, amssymb, physics}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage[colorlinks=true]{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{palatino}
\usepackage{hyperref}
\usepackage{paralist}
\usepackage{todonotes}
\setlength{\marginparwidth}{2.15cm}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,backgrounds}
\setlength\parindent{0pt}

\begin{document}
\vspace*{-1.5cm}
{\centering \vbox{%
\vspace{2mm}
\large
Engineering Mathematics 2 \hfill
\\
Seoul National University
\\[4mm]
Homework 7 \\
\textbf{2021-16988 Jaewan Park} \\[0.8mm]
}}
\par\noindent\rule{\textwidth}{0.5pt}

\setstretch{1.3}
\section*{Exercise 7.2}
The answer is $P_{0, 0}^{t} = \dfrac{1 + (2p-1)^t}{2}$. 
We can prove this by mathematical induction. 
When $t = 1$, we have $P_{0, 0}^{1} = p$, so the claim holds.
Now suppose the claim holds at $t$. 
Then $P_{0, 1}^{t} = 1 - P_{0, 0}^{t} = \dfrac{1 - (2p-1)^t}{2}$, so
\begin{align*}
    P_{0, 0}^{t+1} &= P_{0, 0}P_{0, 0}^{t} + P_{0, 1}P_{1, 0}^{t} \\
    &= \frac{p\qty(1 + (2p-1)^t)}{2} + \frac{(1-p)\qty(1 - (2p-1)^t)}{2} \\
    &= \frac{1 + (2p-1)^{t+1}}{2}.
\end{align*}
Therefore the claim holds for all $t$.

\section*{Exercise 7.6}
Let $h_i$ the expected number of moves to reach $n$ starting from $i$.
Then we obtain
\begin{gather*}
    h_0 = \frac{1}{2}h_0 + \frac{1}{2}h_1 + 1 \\
    h_1 = \frac{1}{2}c_0 + \frac{1}{2}h_2 + 1 \\
    \vdots \\
    h_{n-1} = \frac{1}{2}h_{n-2} + \frac{1}{2}h_n + 1 \\
    h_n = 0
\end{gather*}
For $i = 0, \cdots, n-1$, we can say $h_i = h_{i+1} + 2(i+1)$. 
We can prove this by mathematical induction.
When $i=0$, we have $h_0 = h_1 + 2$, so the claim holds.
Now suppose the claim holds at $i < n-1$. Then
$$h_{i+1} = \frac{1}{2}h_{i} + \frac{1}{2}h_{i+2} + 1 = \frac{1}{2}h_{i+1} + (i+1) + \frac{1}{2}h_{i+2} + 1$$
$$h_{i+1} = h_{i+2} + 2(i+2)$$
Therefore the claim holds for all $i$. 
Since $h_n = 0$, we obtain
$$h_{i} = h_{i+1} + 2(i+1) = \cdots = h_{n} + 2(i+1) + 2(i+2) + \cdots + 2n = (n+i+1)(n-i)$$
which is the expected number of moves to reach $n$ from $i$.

\section*{Exercise 7.12}
Define $Z_i = X_i \;(\mathrm{mod}\; k)$, then the sequence of $Z_i$ becomes a Markov chain since probabilities on $Z_i$ can be obtained just from the previous state.
Let $P$ be the corresponding probability matrix. 
Consider the uniform distribution $\vec{\pi} = \qty(\dfrac{1}{k}, \dfrac{1}{k}, \cdots, \dfrac{1}{k})$, then for all $j$ we obtain
$$\qty(\vec{\pi}P)_{j} = \sum_{i=0}^{k-1}{\pi}_iP_{i, j} = \frac{1}{k}\sum_{i=0}^{k-1}P_{i, j} = \frac{1}{k} = {\pi}_{j}.$$
Therefore $\vec{\pi}$ is a stationary distribution of the chain.
Finally we can say
$$\lim_{n \to \infty}\mathrm{Pr}\qty(X_n \text{ is divisible by } k) = \lim_{n \to \infty}\mathrm{Pr}\qty(Z_n = 0) = \lim_{t \to \infty}P_{0, 0}^{t} = \frac{1}{h_{i, i}} = \pi_i = \frac{1}{k}.$$

\section*{Exercise 7.13}
\begin{enumerate}[(a)]
    \item Using properties of Markov chains, we obtain the following.
    \begin{align*}
        &\hspace{-7mm} \mathrm{Pr}\qty(X_k = a_k \,|\, X_{k+1} = a_{k+1}, X_{k+2} = a_{k+2}, \cdots, X_m = a_m) \\
        &= \frac{\mathrm{Pr}\qty(X_k = a_k, \cdots, X_m = a_m)}{\mathrm{Pr}\qty(X_{k+1} = a_{k+1}, \cdots, X_m = a_m)} \\
        &= \frac{\mathrm{Pr}\qty(X_k = a_k, X_{k+1} = a_{k+1})\mathrm{Pr}\qty(X_{k+2} = a_{k+2}, \cdots, X_m = a_m \,|\, X_k = a_k, X_{k+1} = a_{k+1})}{\mathrm{Pr}\qty(X_{k+1} = a_{k+1})\mathrm{Pr}\qty(X_{k+2} = a_{k+2}, \cdots, X_m = a_m \,|\, X_{k+1} = a_{k+1})} \\
        &= \frac{\mathrm{Pr}\qty(X_k = a_k, X_{k+1} = a_{k+1})}{\mathrm{Pr}\qty(X_{k+1} = a_{k+1})} = \mathrm{Pr}\qty(X_k = a_k \,|\, X_{k+1} = a_{k+1})
    \end{align*}
    Therefore the reverse sequence is Markovian.
    \item The stationary distribution gives $\mathrm{Pr}\qty(X_k = j) = \pi_j$ for all $j$ and $k$. Therefore
    \begin{align*}
        Q_{i, j} &= \mathrm{Pr}\qty(X_k = j \,|\, X_{k+1} = i) \\
        &= \frac{\mathrm{Pr}\qty(X_k = j)\mathrm{Pr}\qty(X_{k+1} = i \,|\, X_k = j)}{\mathrm{Pr}\qty(X_{k+1} = i)} \\
        &= \frac{\pi_jP_{j, i}}{\pi_i}.
    \end{align*}
    \item If $\pi_iP_{i, j} = \pi_jP_{j, i}$, simply substituting this equation to the result of (b) gives the following.
    $$Q_{i, j} = \frac{\pi_jP_{j, i}}{\pi_i} = \frac{\pi_iP_{i, j}}{\pi_i} = P_{i, j}$$
\end{enumerate}

\section*{Exercise 7.17}
When starting from an arbitrary position, moving backward ensures that it will come back to that position, since we cannot move lower than $0$. 
Therefore we can simplify the problem by thinking all situations as starting from $0$.
When we start fom $0$, the probability that we come back to $0$ at the first time in $2n+2$ moves is given by
$$\frac{1}{n+1}{2n \choose n}p^n(1-p)^{n+1}.$$
This comes from that once we hit $0$, we must move to $1$, and the number of paths that move from $1$ to $1$ without hitting $0$ is $\displaystyle \frac{1}{n+1}{2n \choose n}$, and we should move forward $n$ times and backward $n+1$ times.
Then the probability that we first return to $0$ after $t$ moves is
$$r_{0, 0}^{t} = \sum_{n=0}^{\infty}\frac{1}{n+1}{2n \choose n}p^n(1-p)^{n+1} = (1-p)\sum_{n=0}^{\infty}C_n(p(1-p))^n = (1-p)\frac{1 - \sqrt{1 - 4p(1-p)}}{2p(1-p)} = \frac{1 - \qty|1-2p|}{2p}.$$
\begin{enumerate}[(a)]
    \item If $p < \dfrac{1}{2}$, $r_{0, 0}^t = 1$, so the chain is recurrent.
    We can determine whether this is positive or negative recurrent using the following equation. 
    $$h_{0, 0}^t = \sum_{n=0}^{\infty}(2n+2)\frac{1}{n+1}{2n \choose n}p^n(1-p)^{n+1} = 2(1-p)\sum_{n=0}^{\infty}{2n \choose n}(p(1-p))^n = \frac{2(1-p)}{\sqrt{1 - 4p(1-p)}}$$
    Since $h_{0, 0}^t$ is finite when $p < \dfrac{1}{2}$, the chain is positive recurrent.
    \item If $p = \dfrac{1}{2}$, $r_{0, 0}^t = 1$, so the chain is recurrent.
    Since $h_{0, 0}^t$ is infinite when $p = \dfrac{1}{2}$, the chain is negative recurrent.
    \item If $p > \dfrac{1}{2}$, $r_{0, 0}^t = 1 = \dfrac{1-p}{p} < 1$, so the chain is transient.
\end{enumerate}

\section*{Exercise 7.21}
Let $\vec{\pi}$ a stationary distribution of the given Markov chain.  
Since $P_{i, 0} = 1/2$ for $i \leq n$, we have
$${\pi}_{0} = \qty(\vec{\pi}P)_{0} = \sum_{i=0}^{n}{\pi}_iP_{i, 0} = \frac{1}{2}\sum_{i=0}^{n}\pi_i = \frac{1}{2}.$$
Now from $P_{i, i+1} = 1/2$, we obtain
$$\pi_1 = \qty(\vec{\pi}P)_{1} = \sum_{i=0}^{n}{\pi}_iP_{i, 1} = \pi_0P_{0, 1} = \frac{1}{2^2}$$
$$\pi_2 = \qty(\vec{\pi}P)_{2} = \sum_{i=0}^{n}{\pi}_iP_{i, 2} = \pi_1P_{1, 2} = \frac{1}{2^3}$$
$$\vdots$$
$$\pi_{n-1} = \qty(\vec{\pi}P)_{n-1} = \sum_{i=0}^{n}{\pi}_iP_{i, n-1} = \pi_{n-2}P_{n-2, n-1} = \frac{1}{2^n}.$$
From $P_{n, n} = 1/2$, we obtain
$$\pi_{n} = \qty(\vec{\pi}P)_{n} = \sum_{i=0}^{n}{\pi}_iP_{i, n} = \pi_{n-1}P_{n-1, n} + \pi_{n}P_{n, n} = \frac{1}{2^{n+1}} + \frac{1}{2}\pi_n, \;\;\;\; \therefore \pi_n = \frac{1}{2^n}$$
Therefore the stationary distribution is $\vec{\pi} = \qty(\dfrac{1}{2}, \dfrac{1}{2^2}, \cdots, \dfrac{1}{2^n}, \dfrac{1}{2^n})$.

\end{document}